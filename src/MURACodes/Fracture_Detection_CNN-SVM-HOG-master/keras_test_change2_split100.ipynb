{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/var/pyenv/versions/3.7.4/envs/env374/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 2\n",
    "epochs = 700\n",
    "data_augmentation = True\n",
    "num_predictions = 2\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'Keras_change_700_1phase_split100.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imHog_list = []\n",
    "im_flt = []\n",
    "\n",
    "with open('/home/xiaoran/Dropbox/svm_Over3/flipnames.txt') as f:\n",
    "    nn = f.readlines()\n",
    "    names = [x.strip() for x in nn] \n",
    "\n",
    "num = 0    \n",
    "for name in names:\n",
    "    img = cv2.imread('/home/xiaoran/Dropbox/svm_Over3/resize/'+name)\n",
    "#     img = ToZero('/home/xiaoran/Dropbox/ANN/resize/'+name)\n",
    "#    im = cv2.imread('/home/xiaoran/Dropbox/svm/resized/'+name)\n",
    "#    img = cv2.cvtColor( im, cv2.COLOR_RGB2GRAY )\n",
    "#    fd, hog_image = hog(img, orientations= 12, pixels_per_cell=(12, 12),\n",
    "#                    cells_per_block=(1, 1), visualise=True)\n",
    "    fd = img.flatten()\n",
    "    im_flt.append(fd)\n",
    "    imHog_list.append(img)\n",
    "    num = num + 1\n",
    "\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_array = np.zeros((len(names),1))\n",
    "with open('/home/xiaoran/Dropbox/svm_Over3/flipLabel_values.txt') as f:\n",
    "    vals = f.readlines()\n",
    "    vals = [x.strip() for x in vals] \n",
    "k = 0 \n",
    "for val in vals:\n",
    "    label_array[k,0] = val\n",
    "    k = k+1\n",
    "   \n",
    "im_array = np.asarray(imHog_list)\n",
    "im_flt_arr = np.asarray(im_flt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# misMatch = [76,78,84,277,281,306,380,382,393,534,674,1229,1289,1310,1322,1427,1436,1553,1619,1637,1640,1697,\n",
    "#                      1706,1745,1766,1775,1874,1985,2033,2057,2078,2126,2174,2177,2225,2291]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (1365, 1)\n",
      "x_train shape: (1365, 32, 32, 3)\n",
      "X shape: (2097, 32, 32, 3)\n",
      "X_lr shape: (147, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# assign variables to X and y\n",
    "Xraw = im_array\n",
    "yraw = label_array\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# sc.fit(Xraw)\n",
    "# Xraw = sc.transform(Xraw)\n",
    "\n",
    "n = np.arange(500,500)\n",
    "\n",
    "X = np.delete(Xraw, n, axis=0)\n",
    "y = np.delete(yraw, n, axis=0)\n",
    "X_r = X[:50]\n",
    "y_r = y[:50]\n",
    "X_left = X[2000:]\n",
    "y_left = y[2000:]\n",
    "\n",
    "X_lr = np.concatenate((X_r, X_left), axis=0)\n",
    "y_lr = np.concatenate((y_r, y_left), axis=0)\n",
    "\n",
    "# for i in misMatch:\n",
    "#     y[i] = 1 - y[i]\n",
    "# split training set and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X[50:2000], y[50:2000], test_size=0.3, random_state=42)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('X shape:', X.shape)\n",
    "X_out = im_flt_arr \n",
    "print('X_lr shape:', X_lr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.savetxt(\"/home/xiaoran/Dropbox/CNN/yraw.csv\",y, delimiter=',', header=\"y\", comments=\"\")\n",
    "# # np.savetxt(\"/home/xiaoran/Dropbox/CNN/Xraw.csv\",X, delimiter=',', header=\"X\", comments=\"\")\n",
    "\n",
    "# import csv\n",
    "\n",
    "# with open(\"/home/xiaoran/Dropbox/CNN/X_out.csv\",'w', newline=\"\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(X_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (1365, 2)\n",
      "y_left shape: (97, 2)\n"
     ]
    }
   ],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_left = keras.utils.to_categorical(y_left, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_left shape:', y_left.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "# #########add a new layer####################\n",
    "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Conv2D(64, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "#######################################################\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/700\n",
      "43/43 [==============================] - 7s 173ms/step - loss: 6.2276 - acc: 0.5460 - val_loss: 2.0353 - val_acc: 0.8530\n",
      "Epoch 2/700\n",
      "43/43 [==============================] - 5s 128ms/step - loss: 4.6724 - acc: 0.6471 - val_loss: 2.2745 - val_acc: 0.8581\n",
      "Epoch 3/700\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 3.6230 - acc: 0.7223 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 4/700\n",
      "43/43 [==============================] - 6s 146ms/step - loss: 3.1881 - acc: 0.7659 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 5/700\n",
      "43/43 [==============================] - 8s 179ms/step - loss: 2.9256 - acc: 0.7848 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 6/700\n",
      "43/43 [==============================] - 6s 141ms/step - loss: 3.0071 - acc: 0.7884 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 7/700\n",
      "43/43 [==============================] - 6s 137ms/step - loss: 2.7041 - acc: 0.8037 - val_loss: 2.2865 - val_acc: 0.8581\n",
      "Epoch 8/700\n",
      "43/43 [==============================] - 6s 148ms/step - loss: 2.6740 - acc: 0.8106 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 9/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 2.7250 - acc: 0.8128 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 10/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 2.6013 - acc: 0.8204 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 11/700\n",
      "43/43 [==============================] - 5s 123ms/step - loss: 2.6595 - acc: 0.8124 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 12/700\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 2.6704 - acc: 0.8164 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 13/700\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 2.6305 - acc: 0.8139 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 14/700\n",
      "43/43 [==============================] - 6s 129ms/step - loss: 2.6164 - acc: 0.8179 - val_loss: 2.2870 - val_acc: 0.8581\n",
      "Epoch 15/700\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 2.6696 - acc: 0.8190 - val_loss: 2.2869 - val_acc: 0.8581\n",
      "Epoch 16/700\n",
      "43/43 [==============================] - 7s 158ms/step - loss: 2.5061 - acc: 0.8164 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 17/700\n",
      "43/43 [==============================] - 5s 120ms/step - loss: 2.6180 - acc: 0.8295 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 18/700\n",
      "43/43 [==============================] - 5s 128ms/step - loss: 2.6195 - acc: 0.8270 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 19/700\n",
      "43/43 [==============================] - 6s 129ms/step - loss: 2.6071 - acc: 0.8317 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 20/700\n",
      "43/43 [==============================] - 6s 133ms/step - loss: 2.6038 - acc: 0.8252 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 21/700\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 2.4964 - acc: 0.8240 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 22/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 2.3946 - acc: 0.8379 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 23/700\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 2.4335 - acc: 0.8291 - val_loss: 2.2854 - val_acc: 0.8564\n",
      "Epoch 24/700\n",
      "43/43 [==============================] - 6s 136ms/step - loss: 2.5153 - acc: 0.8270 - val_loss: 2.2870 - val_acc: 0.8581\n",
      "Epoch 25/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 2.5715 - acc: 0.8234 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 26/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 2.5947 - acc: 0.8306 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 27/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 2.5025 - acc: 0.8339 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 28/700\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 2.5429 - acc: 0.8302 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 29/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 2.4742 - acc: 0.8306 - val_loss: 2.2846 - val_acc: 0.8564\n",
      "Epoch 30/700\n",
      "43/43 [==============================] - 5s 122ms/step - loss: 2.4550 - acc: 0.8237 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 31/700\n",
      "43/43 [==============================] - 5s 120ms/step - loss: 2.5046 - acc: 0.8342 - val_loss: 2.2868 - val_acc: 0.8581\n",
      "Epoch 32/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 2.4098 - acc: 0.8295 - val_loss: 2.2580 - val_acc: 0.8547\n",
      "Epoch 33/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 2.3828 - acc: 0.8124 - val_loss: 2.1583 - val_acc: 0.8513\n",
      "Epoch 34/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 2.4272 - acc: 0.8055 - val_loss: 2.1822 - val_acc: 0.8530\n",
      "Epoch 35/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 2.5659 - acc: 0.8008 - val_loss: 2.2371 - val_acc: 0.8547\n",
      "Epoch 36/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 2.4268 - acc: 0.8048 - val_loss: 2.2602 - val_acc: 0.8547\n",
      "Epoch 37/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 2.2547 - acc: 0.8291 - val_loss: 2.2308 - val_acc: 0.8547\n",
      "Epoch 38/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 2.3611 - acc: 0.8179 - val_loss: 1.9874 - val_acc: 0.8427\n",
      "Epoch 39/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 2.3097 - acc: 0.8103 - val_loss: 2.1189 - val_acc: 0.8530\n",
      "Epoch 40/700\n",
      "43/43 [==============================] - 5s 120ms/step - loss: 2.1938 - acc: 0.8255 - val_loss: 2.0184 - val_acc: 0.8530\n",
      "Epoch 41/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 2.2634 - acc: 0.8153 - val_loss: 2.0875 - val_acc: 0.8547\n",
      "Epoch 42/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 2.2728 - acc: 0.7925 - val_loss: 2.0801 - val_acc: 0.8547\n",
      "Epoch 43/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 2.2297 - acc: 0.8248 - val_loss: 2.0960 - val_acc: 0.8547\n",
      "Epoch 44/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 2.1580 - acc: 0.8052 - val_loss: 1.6151 - val_acc: 0.8342\n",
      "Epoch 45/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 2.1424 - acc: 0.8023 - val_loss: 2.1026 - val_acc: 0.8530\n",
      "Epoch 46/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 2.0523 - acc: 0.8230 - val_loss: 1.7163 - val_acc: 0.8530\n",
      "Epoch 47/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 1.8952 - acc: 0.7990 - val_loss: 1.5260 - val_acc: 0.8444\n",
      "Epoch 48/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 2.0904 - acc: 0.7921 - val_loss: 1.6150 - val_acc: 0.8547\n",
      "Epoch 49/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 1.8961 - acc: 0.7979 - val_loss: 1.1102 - val_acc: 0.8068\n",
      "Epoch 50/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 1.9089 - acc: 0.7993 - val_loss: 1.0907 - val_acc: 0.8239\n",
      "Epoch 51/700\n",
      "43/43 [==============================] - 6s 133ms/step - loss: 1.7473 - acc: 0.7921 - val_loss: 1.2648 - val_acc: 0.8530\n",
      "Epoch 52/700\n",
      "43/43 [==============================] - 6s 129ms/step - loss: 1.6336 - acc: 0.7917 - val_loss: 0.8038 - val_acc: 0.8359\n",
      "Epoch 53/700\n",
      "43/43 [==============================] - 7s 174ms/step - loss: 1.4412 - acc: 0.7928 - val_loss: 0.8595 - val_acc: 0.8564\n",
      "Epoch 54/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 1.3521 - acc: 0.7954 - val_loss: 0.5409 - val_acc: 0.8376\n",
      "Epoch 55/700\n",
      "43/43 [==============================] - 9s 201ms/step - loss: 1.2885 - acc: 0.7714 - val_loss: 0.5126 - val_acc: 0.8462\n",
      "Epoch 56/700\n",
      "43/43 [==============================] - 9s 220ms/step - loss: 1.1012 - acc: 0.7859 - val_loss: 0.4545 - val_acc: 0.8444\n",
      "Epoch 57/700\n",
      "43/43 [==============================] - 29s 678ms/step - loss: 0.8817 - acc: 0.7939 - val_loss: 0.4532 - val_acc: 0.8547\n",
      "Epoch 58/700\n",
      "43/43 [==============================] - 7s 154ms/step - loss: 0.7906 - acc: 0.7968 - val_loss: 0.3989 - val_acc: 0.8530\n",
      "Epoch 59/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.7746 - acc: 0.7957 - val_loss: 0.3780 - val_acc: 0.8615\n",
      "Epoch 60/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.7515 - acc: 0.7873 - val_loss: 0.3678 - val_acc: 0.8513\n",
      "Epoch 61/700\n",
      "43/43 [==============================] - 6s 140ms/step - loss: 0.7059 - acc: 0.7834 - val_loss: 0.3661 - val_acc: 0.8547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.7044 - acc: 0.7797 - val_loss: 0.3561 - val_acc: 0.8581\n",
      "Epoch 63/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.6406 - acc: 0.7982 - val_loss: 0.3678 - val_acc: 0.8496\n",
      "Epoch 64/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.5683 - acc: 0.8001 - val_loss: 0.4226 - val_acc: 0.7966\n",
      "Epoch 65/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.5627 - acc: 0.8056 - val_loss: 0.4386 - val_acc: 0.7863\n",
      "Epoch 66/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4816 - acc: 0.8084 - val_loss: 0.3842 - val_acc: 0.8496\n",
      "Epoch 67/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.5287 - acc: 0.8026 - val_loss: 0.4238 - val_acc: 0.8034\n",
      "Epoch 68/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4962 - acc: 0.8164 - val_loss: 0.3920 - val_acc: 0.8496\n",
      "Epoch 69/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4752 - acc: 0.8157 - val_loss: 0.4232 - val_acc: 0.8051\n",
      "Epoch 70/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4724 - acc: 0.8099 - val_loss: 0.4054 - val_acc: 0.8342\n",
      "Epoch 71/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.4920 - acc: 0.7990 - val_loss: 0.4339 - val_acc: 0.8017\n",
      "Epoch 72/700\n",
      "43/43 [==============================] - 6s 151ms/step - loss: 0.4598 - acc: 0.8128 - val_loss: 0.4184 - val_acc: 0.8291\n",
      "Epoch 73/700\n",
      "43/43 [==============================] - 5s 127ms/step - loss: 0.4984 - acc: 0.8106 - val_loss: 0.4214 - val_acc: 0.8222\n",
      "Epoch 74/700\n",
      "43/43 [==============================] - 6s 149ms/step - loss: 0.4654 - acc: 0.8143 - val_loss: 0.4369 - val_acc: 0.8068\n",
      "Epoch 75/700\n",
      "43/43 [==============================] - 6s 140ms/step - loss: 0.4588 - acc: 0.8073 - val_loss: 0.4019 - val_acc: 0.8564\n",
      "Epoch 76/700\n",
      "43/43 [==============================] - 6s 138ms/step - loss: 0.4656 - acc: 0.8179 - val_loss: 0.4252 - val_acc: 0.8393\n",
      "Epoch 77/700\n",
      "43/43 [==============================] - 7s 155ms/step - loss: 0.4304 - acc: 0.8317 - val_loss: 0.4201 - val_acc: 0.8496\n",
      "Epoch 78/700\n",
      "43/43 [==============================] - 6s 144ms/step - loss: 0.4622 - acc: 0.8085 - val_loss: 0.3978 - val_acc: 0.8547\n",
      "Epoch 79/700\n",
      "43/43 [==============================] - 7s 166ms/step - loss: 0.4583 - acc: 0.8197 - val_loss: 0.4158 - val_acc: 0.8513\n",
      "Epoch 80/700\n",
      "43/43 [==============================] - 7s 161ms/step - loss: 0.4592 - acc: 0.8142 - val_loss: 0.4091 - val_acc: 0.8530\n",
      "Epoch 81/700\n",
      "43/43 [==============================] - 6s 144ms/step - loss: 0.4529 - acc: 0.8186 - val_loss: 0.4078 - val_acc: 0.8513\n",
      "Epoch 82/700\n",
      "43/43 [==============================] - 5s 120ms/step - loss: 0.4457 - acc: 0.8266 - val_loss: 0.4266 - val_acc: 0.8308\n",
      "Epoch 83/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4421 - acc: 0.8175 - val_loss: 0.4106 - val_acc: 0.8547\n",
      "Epoch 84/700\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 0.4339 - acc: 0.8233 - val_loss: 0.4228 - val_acc: 0.8291\n",
      "Epoch 85/700\n",
      "43/43 [==============================] - 6s 128ms/step - loss: 0.4455 - acc: 0.8165 - val_loss: 0.4314 - val_acc: 0.8085\n",
      "Epoch 86/700\n",
      "43/43 [==============================] - 7s 168ms/step - loss: 0.4550 - acc: 0.8197 - val_loss: 0.4222 - val_acc: 0.8376\n",
      "Epoch 87/700\n",
      "43/43 [==============================] - 6s 142ms/step - loss: 0.4411 - acc: 0.8230 - val_loss: 0.4353 - val_acc: 0.8017\n",
      "Epoch 88/700\n",
      "43/43 [==============================] - 6s 134ms/step - loss: 0.4645 - acc: 0.8171 - val_loss: 0.4092 - val_acc: 0.8513\n",
      "Epoch 89/700\n",
      "43/43 [==============================] - 6s 132ms/step - loss: 0.4583 - acc: 0.8241 - val_loss: 0.4217 - val_acc: 0.8513\n",
      "Epoch 90/700\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 0.4236 - acc: 0.8241 - val_loss: 0.4253 - val_acc: 0.8479\n",
      "Epoch 91/700\n",
      "43/43 [==============================] - 5s 126ms/step - loss: 0.4320 - acc: 0.8361 - val_loss: 0.4355 - val_acc: 0.8154\n",
      "Epoch 92/700\n",
      "43/43 [==============================] - 6s 131ms/step - loss: 0.4378 - acc: 0.8204 - val_loss: 0.4052 - val_acc: 0.8564\n",
      "Epoch 93/700\n",
      "43/43 [==============================] - 6s 145ms/step - loss: 0.4310 - acc: 0.8292 - val_loss: 0.3993 - val_acc: 0.8564\n",
      "Epoch 94/700\n",
      "43/43 [==============================] - 5s 123ms/step - loss: 0.4468 - acc: 0.8350 - val_loss: 0.4213 - val_acc: 0.8564\n",
      "Epoch 95/700\n",
      "43/43 [==============================] - 7s 158ms/step - loss: 0.4205 - acc: 0.8379 - val_loss: 0.4123 - val_acc: 0.8530\n",
      "Epoch 96/700\n",
      "43/43 [==============================] - 7s 160ms/step - loss: 0.4230 - acc: 0.8313 - val_loss: 0.4054 - val_acc: 0.8564\n",
      "Epoch 97/700\n",
      "43/43 [==============================] - 6s 131ms/step - loss: 0.4355 - acc: 0.8240 - val_loss: 0.4178 - val_acc: 0.8547\n",
      "Epoch 98/700\n",
      "43/43 [==============================] - 5s 123ms/step - loss: 0.4471 - acc: 0.8183 - val_loss: 0.4162 - val_acc: 0.8530\n",
      "Epoch 99/700\n",
      "43/43 [==============================] - 6s 143ms/step - loss: 0.4233 - acc: 0.8281 - val_loss: 0.4127 - val_acc: 0.8530\n",
      "Epoch 100/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.4196 - acc: 0.8350 - val_loss: 0.4237 - val_acc: 0.8239\n",
      "Epoch 101/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4456 - acc: 0.8157 - val_loss: 0.4013 - val_acc: 0.8581\n",
      "Epoch 102/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4263 - acc: 0.8273 - val_loss: 0.3988 - val_acc: 0.8496\n",
      "Epoch 103/700\n",
      "43/43 [==============================] - 5s 127ms/step - loss: 0.4379 - acc: 0.8313 - val_loss: 0.4158 - val_acc: 0.8222\n",
      "Epoch 104/700\n",
      "43/43 [==============================] - 6s 143ms/step - loss: 0.4383 - acc: 0.8241 - val_loss: 0.4196 - val_acc: 0.8256\n",
      "Epoch 105/700\n",
      "43/43 [==============================] - 6s 140ms/step - loss: 0.4312 - acc: 0.8273 - val_loss: 0.4194 - val_acc: 0.8342\n",
      "Epoch 106/700\n",
      "43/43 [==============================] - 6s 139ms/step - loss: 0.4488 - acc: 0.8263 - val_loss: 0.4255 - val_acc: 0.8256\n",
      "Epoch 107/700\n",
      "43/43 [==============================] - 7s 152ms/step - loss: 0.4320 - acc: 0.8284 - val_loss: 0.4221 - val_acc: 0.8393\n",
      "Epoch 108/700\n",
      "43/43 [==============================] - 6s 137ms/step - loss: 0.4442 - acc: 0.8262 - val_loss: 0.4108 - val_acc: 0.8496\n",
      "Epoch 109/700\n",
      "43/43 [==============================] - 6s 138ms/step - loss: 0.4606 - acc: 0.8171 - val_loss: 0.4260 - val_acc: 0.8256\n",
      "Epoch 110/700\n",
      "43/43 [==============================] - 6s 134ms/step - loss: 0.4310 - acc: 0.8328 - val_loss: 0.4275 - val_acc: 0.8205\n",
      "Epoch 111/700\n",
      "43/43 [==============================] - 5s 127ms/step - loss: 0.4393 - acc: 0.8310 - val_loss: 0.4059 - val_acc: 0.8564\n",
      "Epoch 112/700\n",
      "43/43 [==============================] - 5s 120ms/step - loss: 0.4327 - acc: 0.8292 - val_loss: 0.4174 - val_acc: 0.8496\n",
      "Epoch 113/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4352 - acc: 0.8223 - val_loss: 0.4183 - val_acc: 0.8444\n",
      "Epoch 114/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4466 - acc: 0.8321 - val_loss: 0.4249 - val_acc: 0.8325\n",
      "Epoch 115/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4414 - acc: 0.8295 - val_loss: 0.4121 - val_acc: 0.8410\n",
      "Epoch 116/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4409 - acc: 0.8273 - val_loss: 0.4219 - val_acc: 0.8274\n",
      "Epoch 117/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4531 - acc: 0.8139 - val_loss: 0.4229 - val_acc: 0.8325\n",
      "Epoch 118/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4480 - acc: 0.8230 - val_loss: 0.4180 - val_acc: 0.8393\n",
      "Epoch 119/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.4552 - acc: 0.8274 - val_loss: 0.4382 - val_acc: 0.8017\n",
      "Epoch 120/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4407 - acc: 0.8310 - val_loss: 0.4208 - val_acc: 0.8393\n",
      "Epoch 121/700\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 0.4323 - acc: 0.8313 - val_loss: 0.4260 - val_acc: 0.8222\n",
      "Epoch 122/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4379 - acc: 0.8295 - val_loss: 0.4520 - val_acc: 0.7658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4420 - acc: 0.8197 - val_loss: 0.4433 - val_acc: 0.7726\n",
      "Epoch 124/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4289 - acc: 0.8252 - val_loss: 0.4280 - val_acc: 0.8205\n",
      "Epoch 125/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4389 - acc: 0.8255 - val_loss: 0.4299 - val_acc: 0.8154\n",
      "Epoch 126/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4379 - acc: 0.8295 - val_loss: 0.4322 - val_acc: 0.8051\n",
      "Epoch 127/700\n",
      "43/43 [==============================] - 5s 123ms/step - loss: 0.4278 - acc: 0.8299 - val_loss: 0.4559 - val_acc: 0.7265\n",
      "Epoch 128/700\n",
      "43/43 [==============================] - 6s 129ms/step - loss: 0.4103 - acc: 0.8252 - val_loss: 0.4181 - val_acc: 0.8239\n",
      "Epoch 129/700\n",
      "43/43 [==============================] - 5s 123ms/step - loss: 0.4186 - acc: 0.8262 - val_loss: 0.4095 - val_acc: 0.8376\n",
      "Epoch 130/700\n",
      "43/43 [==============================] - 7s 156ms/step - loss: 0.4407 - acc: 0.8291 - val_loss: 0.4246 - val_acc: 0.8103\n",
      "Epoch 131/700\n",
      "43/43 [==============================] - 6s 138ms/step - loss: 0.4467 - acc: 0.8259 - val_loss: 0.4214 - val_acc: 0.8239\n",
      "Epoch 132/700\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 0.4605 - acc: 0.8117 - val_loss: 0.4469 - val_acc: 0.7675\n",
      "Epoch 133/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4137 - acc: 0.8321 - val_loss: 0.4272 - val_acc: 0.8034\n",
      "Epoch 134/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4443 - acc: 0.8274 - val_loss: 0.4207 - val_acc: 0.8171\n",
      "Epoch 135/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4191 - acc: 0.8226 - val_loss: 0.4013 - val_acc: 0.8462\n",
      "Epoch 136/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4326 - acc: 0.8313 - val_loss: 0.4224 - val_acc: 0.8256\n",
      "Epoch 137/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4433 - acc: 0.8193 - val_loss: 0.4116 - val_acc: 0.8462\n",
      "Epoch 138/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4207 - acc: 0.8273 - val_loss: 0.4215 - val_acc: 0.8222\n",
      "Epoch 139/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4301 - acc: 0.8306 - val_loss: 0.4233 - val_acc: 0.8239\n",
      "Epoch 140/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4535 - acc: 0.8306 - val_loss: 0.4372 - val_acc: 0.8051\n",
      "Epoch 141/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4449 - acc: 0.8223 - val_loss: 0.4323 - val_acc: 0.8085\n",
      "Epoch 142/700\n",
      "43/43 [==============================] - 5s 120ms/step - loss: 0.4456 - acc: 0.8197 - val_loss: 0.4124 - val_acc: 0.8427\n",
      "Epoch 143/700\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 0.4447 - acc: 0.8252 - val_loss: 0.4164 - val_acc: 0.8393\n",
      "Epoch 144/700\n",
      "43/43 [==============================] - 5s 120ms/step - loss: 0.4362 - acc: 0.8277 - val_loss: 0.4118 - val_acc: 0.8359\n",
      "Epoch 145/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4235 - acc: 0.8328 - val_loss: 0.4170 - val_acc: 0.8239\n",
      "Epoch 146/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4256 - acc: 0.8397 - val_loss: 0.4317 - val_acc: 0.7949\n",
      "Epoch 147/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.4204 - acc: 0.8375 - val_loss: 0.4312 - val_acc: 0.7966\n",
      "Epoch 148/700\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 0.4217 - acc: 0.8324 - val_loss: 0.4174 - val_acc: 0.8256\n",
      "Epoch 149/700\n",
      "43/43 [==============================] - 6s 129ms/step - loss: 0.4272 - acc: 0.8343 - val_loss: 0.4071 - val_acc: 0.8462\n",
      "Epoch 150/700\n",
      "43/43 [==============================] - 6s 133ms/step - loss: 0.4189 - acc: 0.8321 - val_loss: 0.4006 - val_acc: 0.8479\n",
      "Epoch 151/700\n",
      "43/43 [==============================] - 6s 140ms/step - loss: 0.4230 - acc: 0.8292 - val_loss: 0.4017 - val_acc: 0.8462\n",
      "Epoch 152/700\n",
      "43/43 [==============================] - 6s 136ms/step - loss: 0.3946 - acc: 0.8390 - val_loss: 0.4255 - val_acc: 0.8256\n",
      "Epoch 153/700\n",
      "43/43 [==============================] - 5s 126ms/step - loss: 0.4364 - acc: 0.8204 - val_loss: 0.4264 - val_acc: 0.8171\n",
      "Epoch 154/700\n",
      "43/43 [==============================] - 7s 160ms/step - loss: 0.4214 - acc: 0.8321 - val_loss: 0.4201 - val_acc: 0.8188\n",
      "Epoch 155/700\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 0.4032 - acc: 0.8401 - val_loss: 0.4505 - val_acc: 0.7538\n",
      "Epoch 156/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4315 - acc: 0.8259 - val_loss: 0.4432 - val_acc: 0.7744\n",
      "Epoch 157/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4053 - acc: 0.8364 - val_loss: 0.4218 - val_acc: 0.8085\n",
      "Epoch 158/700\n",
      "43/43 [==============================] - 6s 128ms/step - loss: 0.4261 - acc: 0.8357 - val_loss: 0.4338 - val_acc: 0.7880\n",
      "Epoch 159/700\n",
      "43/43 [==============================] - 6s 130ms/step - loss: 0.4472 - acc: 0.8219 - val_loss: 0.4116 - val_acc: 0.8171\n",
      "Epoch 160/700\n",
      "43/43 [==============================] - 5s 125ms/step - loss: 0.4247 - acc: 0.8292 - val_loss: 0.4296 - val_acc: 0.7897\n",
      "Epoch 161/700\n",
      "43/43 [==============================] - 6s 135ms/step - loss: 0.4181 - acc: 0.8364 - val_loss: 0.4125 - val_acc: 0.8205\n",
      "Epoch 162/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4578 - acc: 0.8121 - val_loss: 0.4008 - val_acc: 0.8359\n",
      "Epoch 163/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4407 - acc: 0.8248 - val_loss: 0.4175 - val_acc: 0.8222\n",
      "Epoch 164/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4274 - acc: 0.8277 - val_loss: 0.4100 - val_acc: 0.8308\n",
      "Epoch 165/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4272 - acc: 0.8332 - val_loss: 0.4138 - val_acc: 0.8427\n",
      "Epoch 166/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4297 - acc: 0.8273 - val_loss: 0.3989 - val_acc: 0.8410\n",
      "Epoch 167/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4195 - acc: 0.8408 - val_loss: 0.4248 - val_acc: 0.8068\n",
      "Epoch 168/700\n",
      "43/43 [==============================] - 5s 124ms/step - loss: 0.4209 - acc: 0.8350 - val_loss: 0.4320 - val_acc: 0.8068\n",
      "Epoch 169/700\n",
      "43/43 [==============================] - 6s 149ms/step - loss: 0.4165 - acc: 0.8295 - val_loss: 0.4190 - val_acc: 0.8274\n",
      "Epoch 170/700\n",
      "43/43 [==============================] - 7s 154ms/step - loss: 0.4281 - acc: 0.8324 - val_loss: 0.4376 - val_acc: 0.7915\n",
      "Epoch 171/700\n",
      "43/43 [==============================] - 6s 147ms/step - loss: 0.4381 - acc: 0.8204 - val_loss: 0.4310 - val_acc: 0.8120\n",
      "Epoch 172/700\n",
      "43/43 [==============================] - 6s 141ms/step - loss: 0.4220 - acc: 0.8281 - val_loss: 0.4136 - val_acc: 0.8393\n",
      "Epoch 173/700\n",
      "43/43 [==============================] - 5s 127ms/step - loss: 0.4225 - acc: 0.8357 - val_loss: 0.4180 - val_acc: 0.8342\n",
      "Epoch 174/700\n",
      "43/43 [==============================] - 6s 128ms/step - loss: 0.4239 - acc: 0.8321 - val_loss: 0.3891 - val_acc: 0.8547\n",
      "Epoch 175/700\n",
      "43/43 [==============================] - 6s 150ms/step - loss: 0.4287 - acc: 0.8433 - val_loss: 0.4173 - val_acc: 0.8342\n",
      "Epoch 176/700\n",
      "43/43 [==============================] - 6s 139ms/step - loss: 0.4237 - acc: 0.8343 - val_loss: 0.4154 - val_acc: 0.8256\n",
      "Epoch 177/700\n",
      "43/43 [==============================] - 6s 139ms/step - loss: 0.4140 - acc: 0.8350 - val_loss: 0.4178 - val_acc: 0.8256\n",
      "Epoch 178/700\n",
      "43/43 [==============================] - 6s 144ms/step - loss: 0.4219 - acc: 0.8302 - val_loss: 0.4004 - val_acc: 0.8444\n",
      "Epoch 179/700\n",
      "43/43 [==============================] - 6s 132ms/step - loss: 0.4208 - acc: 0.8346 - val_loss: 0.4341 - val_acc: 0.7880\n",
      "Epoch 180/700\n",
      "43/43 [==============================] - 6s 131ms/step - loss: 0.4301 - acc: 0.8339 - val_loss: 0.4064 - val_acc: 0.8274\n",
      "Epoch 181/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4252 - acc: 0.8404 - val_loss: 0.4172 - val_acc: 0.8137\n",
      "Epoch 182/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4208 - acc: 0.8390 - val_loss: 0.4335 - val_acc: 0.7966\n",
      "Epoch 183/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4235 - acc: 0.8302 - val_loss: 0.3996 - val_acc: 0.8410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4287 - acc: 0.8346 - val_loss: 0.4517 - val_acc: 0.7419\n",
      "Epoch 185/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4207 - acc: 0.8346 - val_loss: 0.4303 - val_acc: 0.7932\n",
      "Epoch 186/700\n",
      "43/43 [==============================] - 5s 122ms/step - loss: 0.4318 - acc: 0.8324 - val_loss: 0.4136 - val_acc: 0.8188\n",
      "Epoch 187/700\n",
      "43/43 [==============================] - 5s 125ms/step - loss: 0.4150 - acc: 0.8408 - val_loss: 0.4399 - val_acc: 0.7812\n",
      "Epoch 188/700\n",
      "43/43 [==============================] - 6s 130ms/step - loss: 0.4170 - acc: 0.8288 - val_loss: 0.4379 - val_acc: 0.7744\n",
      "Epoch 189/700\n",
      "43/43 [==============================] - 6s 130ms/step - loss: 0.4096 - acc: 0.8393 - val_loss: 0.4517 - val_acc: 0.7504\n",
      "Epoch 190/700\n",
      "43/43 [==============================] - 6s 132ms/step - loss: 0.4209 - acc: 0.8350 - val_loss: 0.4234 - val_acc: 0.8068\n",
      "Epoch 191/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4167 - acc: 0.8255 - val_loss: 0.4071 - val_acc: 0.8222\n",
      "Epoch 192/700\n",
      "43/43 [==============================] - 6s 132ms/step - loss: 0.4100 - acc: 0.8361 - val_loss: 0.4321 - val_acc: 0.7897\n",
      "Epoch 193/700\n",
      "43/43 [==============================] - 6s 130ms/step - loss: 0.4145 - acc: 0.8408 - val_loss: 0.4507 - val_acc: 0.7726\n",
      "Epoch 194/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4403 - acc: 0.8277 - val_loss: 0.4206 - val_acc: 0.8137\n",
      "Epoch 195/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4193 - acc: 0.8331 - val_loss: 0.3895 - val_acc: 0.8410\n",
      "Epoch 196/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4087 - acc: 0.8306 - val_loss: 0.3937 - val_acc: 0.8393\n",
      "Epoch 197/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4230 - acc: 0.8360 - val_loss: 0.4031 - val_acc: 0.8342\n",
      "Epoch 198/700\n",
      "43/43 [==============================] - 6s 136ms/step - loss: 0.4244 - acc: 0.8397 - val_loss: 0.3974 - val_acc: 0.8427\n",
      "Epoch 199/700\n",
      "43/43 [==============================] - 6s 141ms/step - loss: 0.4289 - acc: 0.8317 - val_loss: 0.4079 - val_acc: 0.8325\n",
      "Epoch 200/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4231 - acc: 0.8390 - val_loss: 0.3850 - val_acc: 0.8479\n",
      "Epoch 201/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.4271 - acc: 0.8386 - val_loss: 0.3872 - val_acc: 0.8444\n",
      "Epoch 202/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4275 - acc: 0.8430 - val_loss: 0.3968 - val_acc: 0.8393\n",
      "Epoch 203/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.3959 - acc: 0.8408 - val_loss: 0.4315 - val_acc: 0.7949\n",
      "Epoch 204/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4096 - acc: 0.8375 - val_loss: 0.4212 - val_acc: 0.8068\n",
      "Epoch 205/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.4216 - acc: 0.8328 - val_loss: 0.4661 - val_acc: 0.7350\n",
      "Epoch 206/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.4294 - acc: 0.8241 - val_loss: 0.4030 - val_acc: 0.8291\n",
      "Epoch 207/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.4174 - acc: 0.8306 - val_loss: 0.4233 - val_acc: 0.8103\n",
      "Epoch 208/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4060 - acc: 0.8393 - val_loss: 0.4395 - val_acc: 0.7829\n",
      "Epoch 209/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4173 - acc: 0.8328 - val_loss: 0.4365 - val_acc: 0.7932\n",
      "Epoch 210/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.3969 - acc: 0.8343 - val_loss: 0.3688 - val_acc: 0.8427\n",
      "Epoch 211/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.3924 - acc: 0.8346 - val_loss: 0.3815 - val_acc: 0.8359\n",
      "Epoch 212/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.4100 - acc: 0.8397 - val_loss: 0.3931 - val_acc: 0.8291\n",
      "Epoch 213/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.3984 - acc: 0.8404 - val_loss: 0.4188 - val_acc: 0.8085\n",
      "Epoch 214/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.4136 - acc: 0.8313 - val_loss: 0.3943 - val_acc: 0.8376\n",
      "Epoch 215/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4131 - acc: 0.8292 - val_loss: 0.4078 - val_acc: 0.8239\n",
      "Epoch 216/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.4108 - acc: 0.8426 - val_loss: 0.4047 - val_acc: 0.8342\n",
      "Epoch 217/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.3997 - acc: 0.8422 - val_loss: 0.4057 - val_acc: 0.8239\n",
      "Epoch 218/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.3897 - acc: 0.8412 - val_loss: 0.4132 - val_acc: 0.8205\n",
      "Epoch 219/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.4072 - acc: 0.8338 - val_loss: 0.4281 - val_acc: 0.7932\n",
      "Epoch 220/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.4134 - acc: 0.8419 - val_loss: 0.3900 - val_acc: 0.8376\n",
      "Epoch 221/700\n",
      "43/43 [==============================] - 5s 126ms/step - loss: 0.4086 - acc: 0.8350 - val_loss: 0.3779 - val_acc: 0.8376\n",
      "Epoch 222/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4074 - acc: 0.8462 - val_loss: 0.4019 - val_acc: 0.8376\n",
      "Epoch 223/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4072 - acc: 0.8397 - val_loss: 0.4099 - val_acc: 0.8137\n",
      "Epoch 224/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4019 - acc: 0.8382 - val_loss: 0.4197 - val_acc: 0.8085\n",
      "Epoch 225/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4248 - acc: 0.8321 - val_loss: 0.4011 - val_acc: 0.8376\n",
      "Epoch 226/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4135 - acc: 0.8277 - val_loss: 0.4130 - val_acc: 0.8205\n",
      "Epoch 227/700\n",
      "43/43 [==============================] - 6s 147ms/step - loss: 0.4154 - acc: 0.8419 - val_loss: 0.4160 - val_acc: 0.8137\n",
      "Epoch 228/700\n",
      "43/43 [==============================] - 6s 138ms/step - loss: 0.3961 - acc: 0.8426 - val_loss: 0.3919 - val_acc: 0.8393\n",
      "Epoch 229/700\n",
      "43/43 [==============================] - 6s 146ms/step - loss: 0.4126 - acc: 0.8335 - val_loss: 0.4133 - val_acc: 0.8137\n",
      "Epoch 230/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4014 - acc: 0.8372 - val_loss: 0.4159 - val_acc: 0.8103\n",
      "Epoch 231/700\n",
      "43/43 [==============================] - 6s 146ms/step - loss: 0.4131 - acc: 0.8404 - val_loss: 0.4007 - val_acc: 0.8308\n",
      "Epoch 232/700\n",
      "43/43 [==============================] - 6s 134ms/step - loss: 0.4168 - acc: 0.8488 - val_loss: 0.4014 - val_acc: 0.8274\n",
      "Epoch 233/700\n",
      "43/43 [==============================] - 5s 122ms/step - loss: 0.3996 - acc: 0.8357 - val_loss: 0.4075 - val_acc: 0.8239\n",
      "Epoch 234/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.3980 - acc: 0.8393 - val_loss: 0.3820 - val_acc: 0.8496\n",
      "Epoch 235/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4220 - acc: 0.8361 - val_loss: 0.3870 - val_acc: 0.8462\n",
      "Epoch 236/700\n",
      "43/43 [==============================] - 5s 120ms/step - loss: 0.3982 - acc: 0.8408 - val_loss: 0.3968 - val_acc: 0.8308\n",
      "Epoch 237/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.4034 - acc: 0.8313 - val_loss: 0.4214 - val_acc: 0.8051\n",
      "Epoch 238/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.4080 - acc: 0.8419 - val_loss: 0.3892 - val_acc: 0.8359\n",
      "Epoch 239/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4066 - acc: 0.8412 - val_loss: 0.4035 - val_acc: 0.8274\n",
      "Epoch 240/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.3985 - acc: 0.8513 - val_loss: 0.4279 - val_acc: 0.7915\n",
      "Epoch 241/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.3939 - acc: 0.8397 - val_loss: 0.3909 - val_acc: 0.8308\n",
      "Epoch 242/700\n",
      "43/43 [==============================] - 5s 121ms/step - loss: 0.3997 - acc: 0.8415 - val_loss: 0.3838 - val_acc: 0.8376\n",
      "Epoch 243/700\n",
      "43/43 [==============================] - 5s 122ms/step - loss: 0.4173 - acc: 0.8397 - val_loss: 0.4033 - val_acc: 0.8342\n",
      "Epoch 244/700\n",
      "43/43 [==============================] - 5s 123ms/step - loss: 0.4011 - acc: 0.8382 - val_loss: 0.3658 - val_acc: 0.8427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245/700\n",
      "43/43 [==============================] - 5s 122ms/step - loss: 0.4198 - acc: 0.8368 - val_loss: 0.4160 - val_acc: 0.8103\n",
      "Epoch 246/700\n",
      "43/43 [==============================] - 5s 122ms/step - loss: 0.3948 - acc: 0.8473 - val_loss: 0.3979 - val_acc: 0.8291\n",
      "Epoch 247/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4075 - acc: 0.8444 - val_loss: 0.3854 - val_acc: 0.8376\n",
      "Epoch 248/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.3845 - acc: 0.8375 - val_loss: 0.3764 - val_acc: 0.8410\n",
      "Epoch 249/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4030 - acc: 0.8423 - val_loss: 0.4177 - val_acc: 0.8085\n",
      "Epoch 250/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4266 - acc: 0.8350 - val_loss: 0.3808 - val_acc: 0.8410\n",
      "Epoch 251/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.4007 - acc: 0.8430 - val_loss: 0.4251 - val_acc: 0.7932\n",
      "Epoch 252/700\n",
      "43/43 [==============================] - 5s 126ms/step - loss: 0.4077 - acc: 0.8295 - val_loss: 0.3780 - val_acc: 0.8444\n",
      "Epoch 253/700\n",
      "43/43 [==============================] - 5s 123ms/step - loss: 0.3888 - acc: 0.8393 - val_loss: 0.3588 - val_acc: 0.8513\n",
      "Epoch 254/700\n",
      "43/43 [==============================] - 5s 120ms/step - loss: 0.3921 - acc: 0.8433 - val_loss: 0.4125 - val_acc: 0.8188\n",
      "Epoch 255/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.3963 - acc: 0.8419 - val_loss: 0.3999 - val_acc: 0.8291\n",
      "Epoch 256/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.3922 - acc: 0.8404 - val_loss: 0.3948 - val_acc: 0.8325\n",
      "Epoch 257/700\n",
      "43/43 [==============================] - 5s 126ms/step - loss: 0.3884 - acc: 0.8459 - val_loss: 0.4314 - val_acc: 0.7846\n",
      "Epoch 258/700\n",
      "43/43 [==============================] - 5s 127ms/step - loss: 0.4106 - acc: 0.8346 - val_loss: 0.4077 - val_acc: 0.8205\n",
      "Epoch 259/700\n",
      "43/43 [==============================] - 5s 119ms/step - loss: 0.3971 - acc: 0.8339 - val_loss: 0.4071 - val_acc: 0.8171\n",
      "Epoch 260/700\n",
      "43/43 [==============================] - 6s 133ms/step - loss: 0.4055 - acc: 0.8317 - val_loss: 0.3910 - val_acc: 0.8308\n",
      "Epoch 261/700\n",
      "43/43 [==============================] - 6s 149ms/step - loss: 0.4131 - acc: 0.8448 - val_loss: 0.3886 - val_acc: 0.8308\n",
      "Epoch 262/700\n",
      "43/43 [==============================] - 6s 129ms/step - loss: 0.3903 - acc: 0.8335 - val_loss: 0.3977 - val_acc: 0.8239\n",
      "Epoch 263/700\n",
      "43/43 [==============================] - 6s 150ms/step - loss: 0.4157 - acc: 0.8360 - val_loss: 0.4097 - val_acc: 0.8171\n",
      "Epoch 264/700\n",
      "43/43 [==============================] - 6s 129ms/step - loss: 0.3876 - acc: 0.8470 - val_loss: 0.3924 - val_acc: 0.8376\n",
      "Epoch 265/700\n",
      "43/43 [==============================] - 6s 128ms/step - loss: 0.4084 - acc: 0.8481 - val_loss: 0.4019 - val_acc: 0.8291\n",
      "Epoch 266/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.3963 - acc: 0.8379 - val_loss: 0.3796 - val_acc: 0.8479\n",
      "Epoch 267/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.4213 - acc: 0.8353 - val_loss: 0.3902 - val_acc: 0.8359\n",
      "Epoch 268/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4047 - acc: 0.8444 - val_loss: 0.4069 - val_acc: 0.8205\n",
      "Epoch 269/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.3804 - acc: 0.8466 - val_loss: 0.4353 - val_acc: 0.7761\n",
      "Epoch 270/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4015 - acc: 0.8433 - val_loss: 0.4150 - val_acc: 0.8205\n",
      "Epoch 271/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.4001 - acc: 0.8419 - val_loss: 0.4576 - val_acc: 0.7658\n",
      "Epoch 272/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4007 - acc: 0.8382 - val_loss: 0.3915 - val_acc: 0.8393\n",
      "Epoch 273/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.3935 - acc: 0.8459 - val_loss: 0.3952 - val_acc: 0.8376\n",
      "Epoch 274/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4052 - acc: 0.8321 - val_loss: 0.4217 - val_acc: 0.8017\n",
      "Epoch 275/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.4137 - acc: 0.8375 - val_loss: 0.4071 - val_acc: 0.8239\n",
      "Epoch 276/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.4012 - acc: 0.8353 - val_loss: 0.3730 - val_acc: 0.8479\n",
      "Epoch 277/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.4146 - acc: 0.8379 - val_loss: 0.4045 - val_acc: 0.8291\n",
      "Epoch 278/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.3959 - acc: 0.8360 - val_loss: 0.4132 - val_acc: 0.8085\n",
      "Epoch 279/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.3995 - acc: 0.8441 - val_loss: 0.3981 - val_acc: 0.8274\n",
      "Epoch 280/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.3963 - acc: 0.8469 - val_loss: 0.4111 - val_acc: 0.8154\n",
      "Epoch 281/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.3921 - acc: 0.8477 - val_loss: 0.4001 - val_acc: 0.8274\n",
      "Epoch 282/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.3941 - acc: 0.8477 - val_loss: 0.3810 - val_acc: 0.8393\n",
      "Epoch 283/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.4021 - acc: 0.8423 - val_loss: 0.3869 - val_acc: 0.8376\n",
      "Epoch 284/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.3942 - acc: 0.8426 - val_loss: 0.3801 - val_acc: 0.8496\n",
      "Epoch 285/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.3883 - acc: 0.8506 - val_loss: 0.4045 - val_acc: 0.8274\n",
      "Epoch 286/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.3751 - acc: 0.8477 - val_loss: 0.3887 - val_acc: 0.8410\n",
      "Epoch 287/700\n",
      "43/43 [==============================] - 6s 128ms/step - loss: 0.3993 - acc: 0.8353 - val_loss: 0.3999 - val_acc: 0.8274\n",
      "Epoch 288/700\n",
      "43/43 [==============================] - 6s 132ms/step - loss: 0.4079 - acc: 0.8423 - val_loss: 0.4106 - val_acc: 0.8103\n",
      "Epoch 289/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.3967 - acc: 0.8462 - val_loss: 0.3587 - val_acc: 0.8547\n",
      "Epoch 290/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.3954 - acc: 0.8510 - val_loss: 0.3784 - val_acc: 0.8410\n",
      "Epoch 291/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.3862 - acc: 0.8430 - val_loss: 0.3787 - val_acc: 0.8444\n",
      "Epoch 292/700\n",
      "43/43 [==============================] - 5s 114ms/step - loss: 0.3986 - acc: 0.8459 - val_loss: 0.3883 - val_acc: 0.8376\n",
      "Epoch 293/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.3927 - acc: 0.8426 - val_loss: 0.3526 - val_acc: 0.8547\n",
      "Epoch 294/700\n",
      "43/43 [==============================] - 5s 113ms/step - loss: 0.3857 - acc: 0.8459 - val_loss: 0.3566 - val_acc: 0.8547\n",
      "Epoch 295/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.3939 - acc: 0.8488 - val_loss: 0.3557 - val_acc: 0.8530\n",
      "Epoch 296/700\n",
      "43/43 [==============================] - 5s 115ms/step - loss: 0.3730 - acc: 0.8539 - val_loss: 0.3892 - val_acc: 0.8410\n",
      "Epoch 297/700\n",
      "43/43 [==============================] - 5s 116ms/step - loss: 0.3970 - acc: 0.8419 - val_loss: 0.3721 - val_acc: 0.8462\n",
      "Epoch 298/700\n",
      "43/43 [==============================] - 5s 117ms/step - loss: 0.3975 - acc: 0.8404 - val_loss: 0.3648 - val_acc: 0.8479\n",
      "Epoch 299/700\n",
      "43/43 [==============================] - 5s 118ms/step - loss: 0.3863 - acc: 0.8495 - val_loss: 0.3631 - val_acc: 0.8462\n",
      "Epoch 300/700\n",
      "43/43 [==============================] - 7s 161ms/step - loss: 0.3867 - acc: 0.8480 - val_loss: 0.3750 - val_acc: 0.8462\n",
      "Epoch 301/700\n",
      "43/43 [==============================] - 7s 157ms/step - loss: 0.3924 - acc: 0.8513 - val_loss: 0.3518 - val_acc: 0.8496\n",
      "Epoch 302/700\n",
      "42/43 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8496"
     ]
    }
   ],
   "source": [
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    hist = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "    print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/xiaoran/Dropbox/CNN/saved_models/Keras_change_700_3phase_split100.h5 \n",
      "585/585 [==============================] - 0s 797us/step\n",
      "Test loss: 0.255382733977\n",
      "Test accuracy: 0.917948717949\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585/585 [==============================] - 1s     \n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_classes shape: (585, 2)\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = keras.utils.to_categorical(y_pred_classes, num_classes)\n",
    "print('y_pred_classes shape:', y_pred_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "y_lr_pred = model.predict_classes(X_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_lr_pred shape: (147, 2)\n"
     ]
    }
   ],
   "source": [
    "y_lr_pred = keras.utils.to_categorical(y_lr_pred, num_classes)\n",
    "print('y_lr_pred shape:', y_lr_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_lr_pred shape: (147, 2)\n"
     ]
    }
   ],
   "source": [
    "y_lr = keras.utils.to_categorical(y_lr, num_classes)\n",
    "print('y_lr_pred shape:', y_lr_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  9,  33,  42,  48,  91,  98, 102, 105, 108, 119, 138]),)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = (y_lr == y_lr_pred).all(axis=1)\n",
    "idx = np.where(z == False)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9251700680272109"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-11/147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iidx = [ 15,  30,  45,  48,  64,  66,  86,  91,  93,  99, 102, 105, 119,\n",
    "        126, 133, 138, 142, 143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yy_lr = [ np.where(r==1)[0][0] for r in y_lr ]\n",
    "yy_lr_pred = [ np.where(r==1)[0][0] for r in y_lr_pred ]\n",
    "yy_pred_classes = [ np.where(r==1)[0][0] for r in y_pred_classes ]\n",
    "yy_test = [ np.where(r==1)[0][0] for r in y_test ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzKKVpMIsWkkqzKKVpML+H+mVgmKDjqQ+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18742fa6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the validation confusion matrix\n",
    "cm = confusion_matrix(yy_lr, yy_lr_pred)\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            cmap=\"Set2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD8CAYAAABJsn7AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYgBLUiEGsCQVYgBLUiH/H3i03CS/40hgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1855c7c7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the test confusion matrix\n",
    "cm = confusion_matrix(yy_test, yy_pred_classes)\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            cmap=\"Set2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "for i in iidx:\n",
    "    scipy.misc.imsave('/home/xiaoran/Dropbox/CNN/mismatched/'+str(i)+'.png', x_test[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxel_by_time is (689, 3072)\n",
      "XX_out is (2294, 3072)\n"
     ]
    }
   ],
   "source": [
    "voxel_by_time = x_test.reshape(x_test.shape[0],32*32*3)\n",
    "\n",
    "XX_out = X.reshape(X.shape[0],32*32*3)\n",
    "print('voxel_by_time is', voxel_by_time.shape)\n",
    "print('XX_out is', XX_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxel_by_time is (689, 3072)\n",
      "XX_out is (2294, 3072)\n",
      "img1 is (3072,)\n"
     ]
    }
   ],
   "source": [
    "# img1 = voxel_by_time[23,:]\n",
    "img1 = voxel_by_time[23,:]\n",
    "print('voxel_by_time is', voxel_by_time.shape)\n",
    "print('XX_out is', XX_out.shape)\n",
    "print('img1 is', img1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# img = Image.fromarray(X[0,:], 'RGB')\n",
    "# img.save('my.png')\n",
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# # image = plt.imshow(X[0,:], interpolation='nearest')\n",
    "# import scipy.misc\n",
    "# scipy.misc.imsave('/home/xiaoran/Dropbox/CNN/mismatched/ii.png', X[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2057]),)\n",
      "(array([1229]),)\n",
      "(array([1874]),)\n",
      "(array([2177]),)\n",
      "(array([2033]),)\n",
      "(array([1706]),)\n",
      "(array([674]),)\n",
      "(array([84]),)\n",
      "(array([1322]),)\n",
      "(array([1436]),)\n",
      "(array([76]),)\n",
      "(array([1775]),)\n",
      "(array([2126]),)\n",
      "(array([1553]),)\n",
      "(array([1985]),)\n",
      "(array([2291]),)\n",
      "(array([2174]),)\n",
      "(array([281]),)\n",
      "(array([1697]),)\n",
      "(array([306]),)\n",
      "(array([1745]),)\n",
      "(array([382]),)\n",
      "(array([1637]),)\n",
      "(array([1619]),)\n",
      "(array([1289]),)\n",
      "(array([1310]),)\n",
      "(array([78]),)\n",
      "(array([2225]),)\n",
      "(array([380]),)\n",
      "(array([1640]),)\n",
      "(array([2078]),)\n",
      "(array([1766]),)\n",
      "(array([277]),)\n",
      "(array([393]),)\n",
      "(array([534]),)\n",
      "(array([1427]),)\n"
     ]
    }
   ],
   "source": [
    "for i in iidx2:\n",
    "    img1 = voxel_by_time[i,:]\n",
    "    print(np.where(np.all(XX_out == img1,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8708272859216255"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(689-89)/689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
